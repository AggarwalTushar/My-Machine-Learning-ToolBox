{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the data from CSV file\n",
    "df = pd.read_csv(\"mnist_train.csv\")\n",
    "X=df.iloc[:,1:].values\n",
    "Y=df.iloc[:,:1].values\n",
    "X=X.reshape(X.shape[0],1,28,28)\n",
    "\n",
    "#Taking 2500 points\n",
    "X=X[:2500]\n",
    "Y=Y[:2500]\n",
    "\n",
    "#One-hot encoding the output matrix\n",
    "Y=np.array([[1 if Y[i]==j else 0 for j in range(10)] for i in range(len(Y))])\n",
    "\n",
    "#Dividing the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the CNN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \n",
    "    def __init__(self,no_input_channels,no_output_channels,learning_rate=0.1):\n",
    "        \n",
    "        #Step size\n",
    "        self.learning_rate=learning_rate\n",
    "        \n",
    "        #Storing the number of input Channels\n",
    "        self.no_input_channels=no_input_channels\n",
    "        \n",
    "        #Storing the number of output channels\n",
    "        self.no_output_channels=no_output_channels\n",
    "        \n",
    "        #Generating 3x3 filters based on number of input and output channels\n",
    "        self.weights=np.array([[np.random.randn(3,3) for i in range(self.no_input_channels)] for j in range(self.no_output_channels)])\n",
    "    \n",
    "    #Flattening Function\n",
    "    def flatten(self,x):\n",
    "        return x.ravel()\n",
    "    \n",
    "    #Forward Function\n",
    "    def forward(self,X):\n",
    "        \n",
    "        #Extracting the number of samples\n",
    "        self.no_samples=X.shape[0]\n",
    "        self.X_train=X\n",
    "        \n",
    "        #Initializing the output matrix \n",
    "        out=np.zeros((X.shape[0],self.no_output_channels,X.shape[2],X.shape[3]))\n",
    "        \n",
    "        #Running a loop over the samples\n",
    "        for i in range(self.no_samples):\n",
    "            \n",
    "            #Running a loop over the input channels\n",
    "            for j in range(self.no_input_channels):\n",
    "                \n",
    "                #Padding the current channel\n",
    "                padded_channel=np.pad(X[i][j], 1, mode='constant')\n",
    "                \n",
    "                #Running a loop over the number of output channels\n",
    "                for k in range(self.no_output_channels):\n",
    "                    \n",
    "                    #Performing the convolution\n",
    "                    for m in range(padded_channel.shape[0]-self.weights[k][j].shape[0]+1):\n",
    "                        for n in range(padded_channel.shape[1]-self.weights[k][j].shape[1]+1):\n",
    "                            out[i][k][m][n]+=np.sum(np.multiply(padded_channel[m:m+self.weights[k][j].shape[0],n:n+self.weights[k][j].shape[1]],self.weights[k][j]))\n",
    "         \n",
    "        #Returning the output\n",
    "        return np.array([self.flatten(item) for item in out])\n",
    "    \n",
    "    \n",
    "    def backward(self,derivative_loss_wrt_output):\n",
    "        \n",
    "        #Reshaping the next input gradient\n",
    "        derivative_loss_wrt_output=derivative_loss_wrt_output.reshape(self.no_samples,self.no_output_channels,self.X_train.shape[2],self.X_train.shape[3])\n",
    "        \n",
    "        #Calculating the gradient of Loss with respect to weights\n",
    "        derivative_loss_wrt_weights=np.zeros(self.weights.shape)\n",
    "        for i in range(self.no_samples):\n",
    "            for j in range(self.no_input_channels):\n",
    "                padded_channel=np.pad(self.X_train[i][j], 1, mode='constant')\n",
    "                for k in range(self.no_output_channels):\n",
    "                        for m in range(padded_channel.shape[0]-derivative_loss_wrt_output[i][k].shape[0]+1):\n",
    "                            for n in range(padded_channel.shape[1]-derivative_loss_wrt_output[i][k].shape[1]+1):\n",
    "                                derivative_loss_wrt_weights[k][j][m][n]+=np.sum(np.multiply(padded_channel[m:m+derivative_loss_wrt_output[i][k].shape[0],n:n+derivative_loss_wrt_output[i][k].shape[1]],derivative_loss_wrt_output[i][k]))\n",
    "        \n",
    "        #Updating the weights\n",
    "        self.weights-=self.learning_rate*derivative_loss_wrt_weights\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model for MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0 : 331.507227394308\n",
      "Cost after iteration 1 : 264.5911748085799\n",
      "Cost after iteration 2 : 160.21177606497127\n",
      "Cost after iteration 3 : 84.56189227225194\n",
      "Cost after iteration 4 : 63.01580348901906\n",
      "Cost after iteration 5 : 43.41801521075486\n",
      "Cost after iteration 6 : 34.200279789502225\n",
      "Cost after iteration 7 : 28.158445886634894\n",
      "Cost after iteration 8 : 24.714660597736973\n",
      "Cost after iteration 9 : 21.38938560824486\n",
      "Cost after iteration 10 : 19.157166732022038\n",
      "Cost after iteration 11 : 17.016685085150616\n",
      "Cost after iteration 12 : 15.644549901932331\n",
      "Cost after iteration 13 : 14.572888805051956\n",
      "Cost after iteration 14 : 13.54080533376064\n"
     ]
    }
   ],
   "source": [
    "class mnist_CNN:\n",
    "    def __init__(self):\n",
    "        \n",
    "        #First Layer with 16 output channels\n",
    "        self.layer1=Layer(1,16)\n",
    "        \n",
    "        #Initializing the weights for the last layer(Fully-connected Layer)\n",
    "        self.weights=np.zeros((16*784,10))\n",
    "        \n",
    "        #Number of iterations\n",
    "        self.iterations=15\n",
    "        \n",
    "        #Batch-size\n",
    "        self.batch_size=10\n",
    "    \n",
    "    #Function which takes the data and update the weights\n",
    "    def fit(self,X_train,y_train):\n",
    "        \n",
    "        #Running a loop over the number of iterations\n",
    "        for iter in range(self.iterations):\n",
    "            \n",
    "            #Running a loop over the data\n",
    "            for j in range(0,len(X_train),self.batch_size):\n",
    "                \n",
    "                #Performing the forward function from layer-1\n",
    "                a1=self.layer1.forward(X_train[j:j+self.batch_size,:])\n",
    "                \n",
    "                #Using tanh activation\n",
    "                out1=np.tanh(a1)\n",
    "                \n",
    "                #Calculating the output from fully connected layer\n",
    "                a2=np.dot(out1,self.weights)\n",
    "                \n",
    "                #Using softmax activation\n",
    "                out2=np.exp(a2)/np.sum(np.exp(a2),axis=1).reshape(len(a2),1)\n",
    "                \n",
    "                #Calculating the gradient of loss wrt output from fully connected layer\n",
    "                derivative_loss_wrt_a2=out2-y_train[j:j+self.batch_size]\n",
    "                \n",
    "                #Calculating the gradient of loss wrt weights from fully connected layer\n",
    "                derivative_loss_wrt_weights=np.array([[ np.sum(np.multiply(derivative_loss_wrt_a2[:,j],out1[:,i])) for j in range(10)] for i in range(self.weights.shape[0])])\n",
    "                \n",
    "                #Calculating the gradient of loss wrt tanh activated output from layer-1\n",
    "                derivative_loss_wrt_out1=np.array([[ np.sum(np.multiply(derivative_loss_wrt_a2[i,:],self.weights[j,:])) for j in range(out1.shape[1])] for i in range(out1.shape[0])])\n",
    "                \n",
    "                #Calculating the gradient of loss wrt output from layer-1\n",
    "                derivative_loss_wrt_a1=derivative_loss_wrt_out1*(1-out1**2)\n",
    "                \n",
    "                #Performing the back propogation on layer-1\n",
    "                self.layer1.backward(derivative_loss_wrt_a1)\n",
    "                \n",
    "                #Updating the weights of fully connnected layer\n",
    "                self.weights-=0.001*derivative_loss_wrt_weights\n",
    "            \n",
    "            #Calculating the Softmax cross entropy loss\n",
    "            a1=self.layer1.forward(X_train)  \n",
    "            out1=np.tanh(a1)\n",
    "            a2=np.dot(out1,self.weights)\n",
    "            out2=np.exp(a2)/np.sum(np.exp(a2),axis=1).reshape(a2.shape[0],1)\n",
    "            print(\"Cost after iteration\",iter,':',np.sum(-y_train*np.log(out2)))\n",
    "    \n",
    "    #Funtion the predicts the output for input x\n",
    "    def predict(self,X):\n",
    "        a1=self.layer1.forward(X)  \n",
    "        out1=np.tanh(a1)\n",
    "        a2=np.dot(out1,self.weights)\n",
    "        out2=np.exp(a2)/np.sum(np.exp(a2),axis=1).reshape(a2.shape[0],1)\n",
    "        final=self.encode(out2)\n",
    "        return final\n",
    "    \n",
    "    #Function that makes the maximum value as 1 and the rest entries as 0\n",
    "    def encode(self,a):\n",
    "        final_output=np.zeros_like(a)\n",
    "        final_output[np.arange(len(a)), a.argmax(1)] = 1\n",
    "        return final_output\n",
    "\n",
    "#Creating the model for MNIST data\n",
    "model=mnist_CNN()\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the trained model to get the output for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predicting the output for test data\n",
    "y_pred=model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 91.52\n"
     ]
    }
   ],
   "source": [
    "#Calculating the Accuracy\n",
    "count=0\n",
    "for i in range(len(y_test)):\n",
    "    if np.argmax(y_test[i])==np.argmax(y_pred[i]):\n",
    "        count+=1\n",
    "print(\"ACCURACY:\",count*100/len(X_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
