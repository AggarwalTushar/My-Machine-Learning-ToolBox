{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries \n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating class layer with forward and backward propagation\n",
    "class Layer():\n",
    "    # input_size = number of input neurons (number of features of the input sample)\n",
    "    # output_size = number of output neurons\n",
    "    def __init__(self, input_size, output_size):\n",
    "        #initialising the weights and biases randomly\n",
    "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
    "        self.bias = np.random.rand(1, output_size) - 0.5\n",
    "\n",
    "    # returns output for a given input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data.reshape(1,input_data.size)\n",
    "        #the output for layer forward pass is w.T*X + b \n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    # computes dL/dW, and input error dL/dX for a given output_error=dL/dY\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        #using derived input error dL/dX = output_error*w.T\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        #using derived weights error dL/dW = X.T*output_error\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "\n",
    "        # updated parameters using gradient descent \n",
    "        # del(w) = learning_rate* dL/dW and w_new = w_old - del(w)\n",
    "        self.weights -= learning_rate * weights_error\n",
    "        \n",
    "        #dL/db = dL/dY (output error)\n",
    "        #so, del(bias) = Learning_rate* output_error and b_new = b_old - del(bias)\n",
    "        self.bias -= learning_rate * output_error\n",
    "        return input_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an Activation where we can pass our preferrable or given activation function for each layer\n",
    "class ActivationLayer():\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        #input given will be activation function to be used and its derivative function i.e activation_prime\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    # returns the activated input through forward pass\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        #just passing the input data through the given activation function\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        #In backward pass, returning the gradient (or input error to be fed to backward pass of other layer)\n",
    "        #gradient = activation function derivative for given input * output_error (dL/dY)\n",
    "        return self.activation_prime(self.input) * output_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining activation function and its derivative for further use \n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1-np.tanh(x)**2\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-1.0*x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def linear(x):\n",
    "    return x \n",
    "\n",
    "def linear_prime(x):\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Mean sqaure loss function and its derivative\n",
    "def mean_square_loss(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true-y_pred, 2))\n",
    "\n",
    "def mean_square_loss_prime(y_true, y_pred):\n",
    "    return 2*(y_pred-y_true)/y_true.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a Network class for conneting all the layers and carring out forward and backward propagation\n",
    "class Network:\n",
    "    def __init__(self):\n",
    "        #initialising array for appending layers later\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_prime = None\n",
    "\n",
    "    # adding layer to network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # specifying the loss to get used for further processinf like mean square or cross entropy etc.\n",
    "    def use_loss(self, loss, loss_prime):\n",
    "        # defined loss function in use\n",
    "        self.loss = loss\n",
    "        # loss function derivating in use \n",
    "        self.loss_prime = loss_prime\n",
    "\n",
    "    # predicting the output for given input data\n",
    "    def prediction(self, input_data):\n",
    "        samples = len(input_data)  #number of samples in input data\n",
    "        result = []\n",
    "\n",
    "        # running network over all the samples\n",
    "        for i in range(samples):\n",
    "            # forward propagation\n",
    "            output = input_data[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_propagation(output)\n",
    "            result.append(output)\n",
    "\n",
    "        return result\n",
    "\n",
    "    # train the network\n",
    "    def fitting(self, x_train, y_train, epochs, learning_rate):\n",
    "        samples = len(x_train)\n",
    "\n",
    "        # iterations loop\n",
    "        for i in range(epochs):\n",
    "            # iteration over all the input samples\n",
    "            for j in range(samples):\n",
    "                # forward pass\n",
    "                output = x_train[j]\n",
    "                # passing the output of one layer as the input to other layer\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_propagation(output)\n",
    "\n",
    "                # backward pass\n",
    "                error = self.loss_prime(y_train[j], output)\n",
    "                #passing the gradient/ error to reversed layers in backward pass which in turn updates our weights and biases finally\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward_propagation(error, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the Boston Dataset from sklearn\n",
    "from sklearn import datasets\n",
    "\n",
    "boston = datasets.load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 1, 13)\n"
     ]
    }
   ],
   "source": [
    "#input dataset \n",
    "X = boston.data\n",
    "X = X.reshape(X.shape[0],1,X.shape[1])\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing the X data using min-max \n",
    "for i in range(X.shape[2]):\n",
    "    for j in range(X.shape[0]):\n",
    "        X[:,:,i][j,0] = (X[:,:,i][j,0] - min(X[:,:,i]))/(max(X[:,:,i]) - min(X[:,:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 1)\n"
     ]
    }
   ],
   "source": [
    "# true output y \n",
    "y = boston.target\n",
    "y = y.reshape(y.size, 1)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalising y using min-max\n",
    "for i in range(y.size):\n",
    "    y[i,0] = (y[i,0] - min(y[:,0]))/(max(y[:,0])-min(y[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# splitting dataset into train test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a network for (a) part\n",
    "net1 = Network()\n",
    "net1.add(Layer(13, 1)) # adding layer1 with input neuron same as number of features in each sample (i.e 13) and output neurons as mentioned in the ques(i.e. 1)\n",
    "net1.add(ActivationLayer(linear, linear_prime)) # activating our previous layer1 with linear activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model using training dataset\n",
    "net1.use_loss(mean_square_loss, mean_square_loss_prime) # Mean square loss to be used as Loss in this case\n",
    "net1.fitting(X_train, y_train, epochs=1200, learning_rate=1e-10) #fitting our training data to network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.4072189]]), array([[-0.1160333]]), array([[-0.08069582]]), array([[-0.31608353]]), array([[0.16411197]]), array([[-0.43269683]]), array([[-0.37499409]]), array([[-0.26955179]]), array([[0.09235116]]), array([[-0.39450549]]), array([[-0.41993058]]), array([[-0.58715146]]), array([[-0.35249193]]), array([[-0.48042762]]), array([[-0.1954203]]), array([[-0.43893545]]), array([[-0.52223876]]), array([[-0.38349842]]), array([[-0.35087838]]), array([[-0.44439491]]), array([[-0.49344794]]), array([[0.12634462]]), array([[0.22432972]]), array([[-0.31089985]]), array([[-0.37477409]]), array([[-0.33121564]]), array([[-0.07442016]]), array([[-0.42672223]]), array([[-0.13267866]]), array([[0.01834049]]), array([[-0.10359189]]), array([[-0.12049787]]), array([[-0.28504253]]), array([[-0.21177302]]), array([[-0.48650579]]), array([[-0.15646634]]), array([[-0.40214155]]), array([[0.13004726]]), array([[-0.52813207]]), array([[-0.07258155]]), array([[-0.21018066]]), array([[-0.39035634]]), array([[-0.03291455]]), array([[-0.23513252]]), array([[-0.57884288]]), array([[-0.39469281]]), array([[-0.25276661]]), array([[0.15798879]]), array([[-0.272536]]), array([[-0.15498186]]), array([[-0.38176603]]), array([[-0.3664857]]), array([[0.22878659]]), array([[-0.40146094]]), array([[-0.65846707]]), array([[-0.45842943]]), array([[-0.39418889]]), array([[-0.02901787]]), array([[-0.18235122]]), array([[-0.42897898]]), array([[-0.32030696]]), array([[-0.36709119]]), array([[-0.52945067]]), array([[-0.42702493]]), array([[-0.46318617]]), array([[-0.3631533]]), array([[-0.38312432]]), array([[-0.23325373]]), array([[-0.25806654]]), array([[-0.27120595]]), array([[-0.15400958]]), array([[-0.12907695]]), array([[0.065087]]), array([[-0.39743423]]), array([[-0.46552983]]), array([[-0.28268482]]), array([[-0.33327088]]), array([[-0.27124745]]), array([[-0.20133834]]), array([[-0.3616282]]), array([[-0.31209041]]), array([[-0.24223657]]), array([[0.11348116]]), array([[-0.49798674]]), array([[-0.06032532]]), array([[-0.25961545]]), array([[-0.16383115]]), array([[-0.47026108]]), array([[-0.52720623]]), array([[-0.18573744]]), array([[0.15814356]]), array([[-0.26553824]]), array([[-0.32594731]]), array([[-0.30867603]]), array([[-0.14045097]]), array([[-0.51938377]]), array([[-0.57166]]), array([[-0.41496475]]), array([[-0.00983275]]), array([[-0.12889682]]), array([[-0.22024474]]), array([[-0.39988223]]), array([[-0.2969082]]), array([[0.02010712]]), array([[-0.36366216]]), array([[-0.36346193]]), array([[-0.19371431]]), array([[-0.48637321]]), array([[-0.37632656]]), array([[-0.32628715]]), array([[-0.40661299]]), array([[-0.40296126]]), array([[-0.20792544]]), array([[-0.09188089]]), array([[-0.49409321]]), array([[-0.57593784]]), array([[-0.49990547]]), array([[-0.5763535]]), array([[0.12306103]]), array([[0.23382803]]), array([[-0.08121503]]), array([[-0.54770232]]), array([[-0.31727327]]), array([[-0.19760278]]), array([[-0.35988764]]), array([[0.11154736]]), array([[0.0683171]]), array([[-0.17105142]]), array([[-0.4136206]]), array([[-0.45956697]]), array([[-0.55970567]]), array([[-0.42655069]]), array([[-0.20384273]]), array([[-0.49873102]]), array([[-0.43863634]]), array([[-0.29471948]]), array([[-0.25174449]]), array([[-0.34226175]]), array([[-0.41394325]]), array([[0.46549949]]), array([[-0.29646292]]), array([[-0.26652179]]), array([[-0.40072915]]), array([[-0.28843776]]), array([[-0.4758121]]), array([[-0.09980621]]), array([[-0.46811448]]), array([[0.10752728]]), array([[0.02311516]]), array([[-0.25265657]]), array([[-0.51770028]]), array([[-0.18167641]])]\n"
     ]
    }
   ],
   "source": [
    "# testing our model using test dataset\n",
    "y_pred_a = net1.prediction(X_test)\n",
    "print(y_pred_a)  #predicted output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7139387046743337"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_square_loss(y_test, y_pred_a)  #calculating mean square loss for out predicted output of (a) part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a network for (b) part\n",
    "net2 = Network()\n",
    "net2.add(Layer(13,13)) # adding layer1 with input neuron same as number of features in each sample (i.e 13) and output neurons as mentioned in the ques(i.e. 13)\n",
    "net2.add(ActivationLayer(sigmoid, sigmoid_prime)) # activating our previous layer1 with sigmoid activation function\n",
    "net2.add(Layer(13, 1)) # adding layer2 with input neuron same as number of output neurons in layer1 (i.e 13) and output neurons as mentioned in the ques(i.e. 1)\n",
    "net2.add(ActivationLayer(linear, linear_prime)) # activating our previous layer2 with linear activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model using training dataset\n",
    "net2.use_loss(mean_square_loss, mean_square_loss_prime) # Mean square loss to be used as Loss in this case\n",
    "net2.fitting(X_train, y_train, epochs=1200, learning_rate=1e-10) #fitting our training data to network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.37927085]]), array([[0.37904826]]), array([[0.32987756]]), array([[0.39571931]]), array([[0.25940893]]), array([[0.41929285]]), array([[0.35263419]]), array([[0.38781685]]), array([[0.25408689]]), array([[0.34555893]]), array([[0.33602521]]), array([[0.4088051]]), array([[0.37538963]]), array([[0.41849416]]), array([[0.34331027]]), array([[0.38384299]]), array([[0.40997477]]), array([[0.37525566]]), array([[0.4215443]]), array([[0.42468084]]), array([[0.4231319]]), array([[0.34943254]]), array([[0.33655202]]), array([[0.41255628]]), array([[0.36921957]]), array([[0.42326478]]), array([[0.42288841]]), array([[0.35527183]]), array([[0.32627633]]), array([[0.38137645]]), array([[0.38612713]]), array([[0.33082052]]), array([[0.34454472]]), array([[0.3245009]]), array([[0.36521835]]), array([[0.36034427]]), array([[0.37864827]]), array([[0.3173999]]), array([[0.39996547]]), array([[0.39121282]]), array([[0.32381166]]), array([[0.36982535]]), array([[0.26973385]]), array([[0.38819287]]), array([[0.37252722]]), array([[0.32427845]]), array([[0.33293838]]), array([[0.28154375]]), array([[0.34822138]]), array([[0.34249325]]), array([[0.35751152]]), array([[0.43100551]]), array([[0.21771746]]), array([[0.41590076]]), array([[0.43052816]]), array([[0.40427678]]), array([[0.40507796]]), array([[0.37796838]]), array([[0.46949876]]), array([[0.38257166]]), array([[0.41661766]]), array([[0.44600965]]), array([[0.41975695]]), array([[0.41573174]]), array([[0.43017174]]), array([[0.41358073]]), array([[0.37110093]]), array([[0.45062651]]), array([[0.34598512]]), array([[0.36190807]]), array([[0.32168433]]), array([[0.38234033]]), array([[0.27158703]]), array([[0.39491075]]), array([[0.33747928]]), array([[0.38836242]]), array([[0.41148114]]), array([[0.39894476]]), array([[0.44049958]]), array([[0.36981477]]), array([[0.3415731]]), array([[0.44958879]]), array([[0.25078384]]), array([[0.39026365]]), array([[0.27357234]]), array([[0.44981495]]), array([[0.41011241]]), array([[0.35131328]]), array([[0.38597811]]), array([[0.30902601]]), array([[0.28788561]]), array([[0.28732596]]), array([[0.36921692]]), array([[0.43047675]]), array([[0.31886694]]), array([[0.40846958]]), array([[0.43218148]]), array([[0.37851633]]), array([[0.2803128]]), array([[0.32009234]]), array([[0.45506125]]), array([[0.35118426]]), array([[0.34114228]]), array([[0.27641501]]), array([[0.39077412]]), array([[0.34637734]]), array([[0.30669631]]), array([[0.35382297]]), array([[0.4005057]]), array([[0.36742881]]), array([[0.3482924]]), array([[0.42068414]]), array([[0.32868504]]), array([[0.30770817]]), array([[0.39561079]]), array([[0.42400009]]), array([[0.42862296]]), array([[0.41509746]]), array([[0.32828407]]), array([[0.26102415]]), array([[0.43311139]]), array([[0.37719473]]), array([[0.37544579]]), array([[0.39001554]]), array([[0.35007625]]), array([[0.26665363]]), array([[0.26844877]]), array([[0.33933729]]), array([[0.37899118]]), array([[0.43124485]]), array([[0.42531794]]), array([[0.41560009]]), array([[0.32226831]]), array([[0.34190802]]), array([[0.42221331]]), array([[0.34956751]]), array([[0.43580699]]), array([[0.40196576]]), array([[0.38132328]]), array([[0.20566292]]), array([[0.39971603]]), array([[0.38453822]]), array([[0.37504342]]), array([[0.43107621]]), array([[0.33844575]]), array([[0.27469776]]), array([[0.37784937]]), array([[0.25136293]]), array([[0.28262851]]), array([[0.31340044]]), array([[0.4172294]]), array([[0.29825016]])]\n"
     ]
    }
   ],
   "source": [
    "# testing our model using test dataset\n",
    "y_pred_b = net2.prediction(X_test)\n",
    "print(y_pred_b) #predicted output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06745321656745663"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_square_loss(y_test, y_pred_b) #calculating mean square loss for out predicted output of (b) part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a network for (c) part\n",
    "net3 = Network()\n",
    "net3.add(Layer(13,13)) # adding layer1 with input neuron same as number of features in each sample (i.e 13) and output neurons as mentioned in the ques(i.e. 13)\n",
    "net3.add(ActivationLayer(sigmoid, sigmoid_prime)) # activating our pervious layer1 with sigmoid activation function\n",
    "net3.add(Layer(13,13)) # adding a layer2 with input neuron same as number of output neurons in layer1 (i.e 13) and output neurons as mentioned in the ques(i.e. 13)\n",
    "net3.add(ActivationLayer(sigmoid, sigmoid_prime)) # activating our pervious layer2 with sigmoid activation function\n",
    "net3.add(Layer(13, 1)) # adding a layer3 with input neuron same as number of output neurons in layer2 (i.e 13) and output neurons as mentioned in the ques(i.e. 1)\n",
    "net3.add(ActivationLayer(linear, linear_prime)) # activating our pervious layer3 with linear activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model using training dataset\n",
    "net3.use_loss(mean_square_loss, mean_square_loss_prime) # Mean square loss to be used as Loss in this case\n",
    "net3.fitting(X_train, y_train, epochs=1200, learning_rate=1e-10) #fitting our training data to network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.50573778]]), array([[0.51293537]]), array([[0.50905894]]), array([[0.51146517]]), array([[0.53723796]]), array([[0.51254498]]), array([[0.51502823]]), array([[0.51463652]]), array([[0.50300632]]), array([[0.51686137]]), array([[0.51632813]]), array([[0.51543154]]), array([[0.51469238]]), array([[0.52003883]]), array([[0.54202211]]), array([[0.51594184]]), array([[0.51360211]]), array([[0.51146043]]), array([[0.51177527]]), array([[0.51911398]]), array([[0.52240692]]), array([[0.54269787]]), array([[0.54156275]]), array([[0.51201405]]), array([[0.50506791]]), array([[0.51200786]]), array([[0.51556929]]), array([[0.51705685]]), array([[0.50902537]]), array([[0.54473579]]), array([[0.5142907]]), array([[0.50685059]]), array([[0.51598694]]), array([[0.54139268]]), array([[0.50600121]]), array([[0.51515065]]), array([[0.51686145]]), array([[0.54198788]]), array([[0.51619074]]), array([[0.51397842]]), array([[0.51146068]]), array([[0.5138457]]), array([[0.50064496]]), array([[0.51336354]]), array([[0.51297705]]), array([[0.51528991]]), array([[0.51129848]]), array([[0.49759458]]), array([[0.51320706]]), array([[0.51191985]]), array([[0.51656355]]), array([[0.51339784]]), array([[0.5066007]]), array([[0.51523927]]), array([[0.51691559]]), array([[0.516163]]), array([[0.51558464]]), array([[0.50998789]]), array([[0.51578475]]), array([[0.51561825]]), array([[0.51179539]]), array([[0.51645201]]), array([[0.52077435]]), array([[0.51329383]]), array([[0.52039579]]), array([[0.51251473]]), array([[0.51678257]]), array([[0.51537149]]), array([[0.51301696]]), array([[0.51486728]]), array([[0.50714848]]), array([[0.51483717]]), array([[0.50087882]]), array([[0.51524077]]), array([[0.51244308]]), array([[0.51574298]]), array([[0.51457842]]), array([[0.51715075]]), array([[0.52508238]]), array([[0.51372373]]), array([[0.50687043]]), array([[0.51373027]]), array([[0.5025523]]), array([[0.51759978]]), array([[0.50683503]]), array([[0.51577835]]), array([[0.51707526]]), array([[0.51291158]]), array([[0.51482503]]), array([[0.50812989]]), array([[0.49911028]]), array([[0.51076841]]), array([[0.51731056]]), array([[0.51848073]]), array([[0.50684462]]), array([[0.51410637]]), array([[0.5254704]]), array([[0.51432546]]), array([[0.5062746]]), array([[0.50735961]]), array([[0.51704669]]), array([[0.51476183]]), array([[0.50829961]]), array([[0.50724262]]), array([[0.51491186]]), array([[0.54439084]]), array([[0.51077866]]), array([[0.51249543]]), array([[0.51223813]]), array([[0.51454691]]), array([[0.50613454]]), array([[0.52030509]]), array([[0.5113711]]), array([[0.5401755]]), array([[0.51395064]]), array([[0.51630194]]), array([[0.52185388]]), array([[0.51671509]]), array([[0.54301888]]), array([[0.50022685]]), array([[0.51705513]]), array([[0.51627729]]), array([[0.51601503]]), array([[0.51603772]]), array([[0.51350545]]), array([[0.50260101]]), array([[0.50025369]]), array([[0.54131046]]), array([[0.50568862]]), array([[0.52048297]]), array([[0.5221859]]), array([[0.52021641]]), array([[0.51073926]]), array([[0.51311172]]), array([[0.52043535]]), array([[0.51216535]]), array([[0.52079885]]), array([[0.5154011]]), array([[0.51029657]]), array([[0.527178]]), array([[0.51394401]]), array([[0.51495338]]), array([[0.50548764]]), array([[0.51056004]]), array([[0.51719775]]), array([[0.50835063]]), array([[0.51211763]]), array([[0.50036638]]), array([[0.50428901]]), array([[0.51141677]]), array([[0.51338684]]), array([[0.50844128]])]\n"
     ]
    }
   ],
   "source": [
    "# testing our model using test dataset\n",
    "y_pred_c = net3.prediction(X_test)\n",
    "print(y_pred_c) #predicted output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.040580350409270605"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_square_loss(y_test, y_pred_c) #calculating mean square loss for out predicted output of (c) part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "1. https://towardsdatascience.com/math-neural-network-from-scratch-in-python-d6da9f29ce65\n",
    "2. https://youtube.com/playlist?list=PLQVvvaa0QuDcjD5BAw2DxE6OF2tius3V3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
